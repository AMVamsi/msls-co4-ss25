{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image segmentation â€“ Basics**\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: MSLS CO4 |\n",
    "<b>Version</b>: v1.2 <br><br>\n",
    "<!-- Date: 16.04.2025 -->\n",
    "<!-- Comments: Text refactored -->\n",
    "</div>\n",
    "\n",
    "In this notebook on segmentation, we will explore different approaches to segment hematological images. As a first step, we will attempt to segment the cells using simple thresholding techniques.\n",
    "\n",
    "Several of the concepts discussed here are also covered in this insightful tutorial for the ImageJ/Fiji plugin [MorphoLibJ](https://imagej.net/plugins/morpholibj), which you may find helpful for further reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Let's begin with the usual preparatory steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "#Â Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable this line if you want to use the interactive widgets\n",
    "# It requires the ipympl package to be installed.\n",
    "#%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same images that were used in the previous notebook on preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "img1 = cv.imread(\"../data/images/hematology-baso1.jpg\", cv.IMREAD_COLOR)\n",
    "img2 = cv.imread(\"../data/images/hematology-baso2.jpg\", cv.IMREAD_COLOR)\n",
    "img3 = cv.imread(\"../data/images/hematology-blast1.jpg\", cv.IMREAD_COLOR)\n",
    "\n",
    "img1 = cv.cvtColor(img1, cv.COLOR_BGR2RGB)\n",
    "img2 = cv.cvtColor(img2, cv.COLOR_BGR2RGB)\n",
    "img3 = cv.cvtColor(img3, cv.COLOR_BGR2RGB)\n",
    "\n",
    "tools.show_image_chain([img1, img2, img3], titles=[\"img1\", \"img2\", \"img3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Method 1: Thresholding**\n",
    "\n",
    "We can segment images using basic thresholding techniques. In this example, we explore several thresholding methods available in OpenCV:\n",
    "\n",
    "- **Simple thresholding**: Use [`cv.threshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57)  \n",
    "  (with flags `cv.THRESH_BINARY` or `cv.THRESH_BINARY_INV`)\n",
    "- **Adaptive thresholding**: Use [`cv.adaptiveThreshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#ga72b913f352e4a1b1b397736707afcde3)\n",
    "- **Otsu's thresholding** : Use [`cv.threshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57)  \n",
    "  (with flags `cv.THRESH_BINARY + cv.THRESH_OTSU`)\n",
    "\n",
    "Thresholding segments pixels into foreground and background based on their intensity values, making it a form of *binary* segmentation*. The algorithm compares pixel intensities to a threshold value: Pixel values larger than the threshold are classified as *foreground*, pixels smaller or equal than the threshold are *background*.\n",
    "\n",
    "The threshold can be manually defined or automatically determined (e.g., by Otsuâ€™s method).\n",
    "\n",
    "As preparation, please review the following OpenCV documentation on thresholding methods:  \n",
    "[https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Choose here the image to work with\n",
    "img = img1\n",
    "\n",
    "# 1) Summarize the three different thresholding techniques in own words. Which methods\n",
    "#    use a global threshold, which ones apply a local threshold? \n",
    "\n",
    "# 2) Develop a strategy to segment the white blood cells (purple), the red blood cells \n",
    "# (red) and the background (white/gray). You may want to exploit the fact that we have\n",
    "# colors to work with:\n",
    "tools.show_image_chain([img[:,:,0], img[:,:,1], img[:,:,2]], titles=[\"R\", \"G\", \"B\"])\n",
    "\n",
    "# 3) Identify the different regions using thresholding\n",
    "mask_wbc = ...\n",
    "mask_rbc = ...\n",
    "mask_bg = ...\n",
    "\n",
    "# 4) Visualize the masks. Idea: combine the three masks into an RGB image\n",
    "mask_seg = ...\n",
    "\n",
    "# 5) Discuss your results. What could be improved? What are the limitations of this \n",
    "#    approach? Are the masks mutually exclusive? Are they accurate?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# 1) cv.threshold:              Apply a global threshold to the image.\n",
    "#    cv.adaptiveThreshold:      Apply a local threshold to the image.\n",
    "#    cv.threshold (otsu):       Apply a global threshold to the image using Otsu's method.\n",
    "\n",
    "# 2) Segmentation strategy:\n",
    "#    The information in the three different channels suggests that we can use the\n",
    "#    red channel to segment the red blood cells, the blue channel to segment the white\n",
    "#    blood cells and the luminance channel (or gray channel) to segment the background. \n",
    "#    We can then combine the three masks to obtain the final segmentation.\n",
    "# \n",
    "#    Display the 3 channels. See note below as to why we disable normalization.\n",
    "img = img1\n",
    "tools.show_image_chain([img[:,:,0], img[:,:,1], img[:,:,2]], \n",
    "                       titles=[\"R\", \"G\", \"B\"], normalize=False)\n",
    "\n",
    "# 3) Let's try how this works in practice\n",
    "def segment_blood_cells_thr(img, return_masks=False):\n",
    "\n",
    "    # Smooth the image (to reduce noise)\n",
    "    #img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # -> Convert image to grayscale\n",
    "    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    # -> Extract background using Otsu's method\n",
    "    thr_bg, mask_bg = cv.threshold(gray, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "    # -> Extract red and white blood cells\n",
    "    thr_rbc, mask_rbc = cv.threshold(img[:,:,0], 130, 255, cv.THRESH_BINARY)\n",
    "    thr_wbc, mask_wbc = cv.threshold(img[:,:,2], 140, 255, cv.THRESH_BINARY)\n",
    "\n",
    "    # -> Apply the segmentation logic: \n",
    "    #     - First observe that the background takes high values in the red and blue\n",
    "    #       channels. Thresholding the red and blue channels will also include the \n",
    "    #       background. Furthermore, the blue component sometimes is also present \n",
    "    #       in the red blood cells. Therefore:\n",
    "    #     - Exclude the background from the masks for the red and white blood cells\n",
    "    #     - Exclude the red blood cells from the white blood cells mask. Here,\n",
    "    #       we use the condition that something appears purple if the blue\n",
    "    #       channel is significantly higher than the red channel.\n",
    "\n",
    "    mask_rbc = mask_rbc.astype(bool) & ~mask_bg.astype(bool)\n",
    "    mask_wbc = mask_wbc.astype(bool) & ~mask_bg.astype(bool)\n",
    "    mask_wbc = mask_wbc & (img[:,:,0]*1.1 < img[:,:,2])\n",
    "\n",
    "    #Â Combine the information into a color image.\n",
    "    result = np.ones_like(img) * 255\n",
    "    result[mask_rbc.astype(bool)] = [155, 107, 132]\n",
    "    result[mask_wbc.astype(bool)] = [62, 32, 152]\n",
    "\n",
    "    if return_masks:\n",
    "        return result, mask_bg, mask_rbc, mask_wbc\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "# Compute the segmentation and viusalize the results\n",
    "img = img1\n",
    "ret = segment_blood_cells_thr(img, return_masks=True)\n",
    "result1, mask_bg, mask_rbc, mask_wbc = ret\n",
    "\n",
    "# Visualize the masks\n",
    "tools.show_image_chain([mask_bg, mask_rbc, mask_wbc], \n",
    "                       titles=[\"Background\", \"RBC\", \"WBC\"])\n",
    "# Visualize the results\n",
    "tools.show_image_chain([img, result1], titles=[\"Input: img1\", \"Output: Segmentation\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation looks fairly good, but it is not perfect. The main limitations are:  \n",
    "\n",
    "- The thresholding is very sensitive to the threshold values. Small changes in the threshold can lead to very different results.  \n",
    "- The assumptions and segmentation logic are tailored to the specific images and may not generalize well.  \n",
    "- The masks contain holes and do not segment the cells precisely â€“ this is a limitation of the thresholding approach used.  \n",
    "- Boundary effects are visible (e.g., in red blood cells), indicating that the method is not fully reliable.  \n",
    "- Nearby or overlapping cells may not be distinguishable in the segmentation. This issue is clearly visible in the result for `img3` (see below).\n",
    "\n",
    "We can refine the results by...\n",
    "- ...tuning the thresholding parameters  \n",
    "- ...improving the segmentation logic  \n",
    "- ...smoothing the image (see the commented-out line of code above)  \n",
    "- ...using *morphological operations* to close holes and remove noise (see next tutorial)\n",
    "\n",
    "Note: When displaying images, it is helpful to disable *normalization*, which automatically stretches pixel values to the full range [0, 255]. This normalization is the default behavior of [`plt.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html), which is used internally by our helper function `tools.show_image_chain()`. To accurately inspect grayscale values (e.g., using a color picker), it is better to view the image with its original intensity values.\n",
    "\n",
    "A **color picker** is a tool that shows the color value under the mouse pointer. \n",
    "- *macOS*: [Digital Color Meter](https://support.apple.com/guide/digital-color-meter/welcome/mac) (pre-installed under /System/Applications/Utilities/)  \n",
    "- *Windows:* *Color Picker* as part of the [PowerToys](https://learn.microsoft.com/en-us/windows/powertoys/)  \n",
    "- *Ubuntu:* [Gpick](https://www.gpick.org/)  \n",
    "\n",
    "These tools support multiple color formats and let you copy color values to the clipboard, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the segmentation results also for the other images:\n",
    "result2 = segment_blood_cells_thr(img2)\n",
    "tools.show_image_chain([img2, result2], titles=[\"Input: img2\", \"Output: Segmentation\"]);\n",
    "result3 = segment_blood_cells_thr(img3)\n",
    "tools.show_image_chain([img3, result3], titles=[\"Input: img3\", \"Output: Segmentation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## **Method 2: Color clustering**\n",
    "\n",
    "Instead of segmenting the image into foreground and background, we can attempt to classify different regions based on color similarity. A common approach for this is the [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) algorithm. K-means classifies pixels into a predefined number of clusters based on their color values. Similarity is typically measured using a (Euclidean or non-Euclidean) distance between pixel values. The algorithm operates iteratively:\n",
    "\n",
    "1. Assign each pixel to the nearest cluster center.\n",
    "2. Update each cluster center as the mean of the pixels assigned to it.\n",
    "3. Repeat until the cluster centers converge.\n",
    "\n",
    "Here is a helpful [visualization](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) of how K-means clustering works.\n",
    "\n",
    "\n",
    "**Preparation:** Before you begin, check out these two tutorials:\n",
    "- Jason Brownlee (Machine Learning Mastery) on [color quantization with K-means](https://machinelearningmastery.com/k-means-clustering-in-opencv-and-application-for-color-quantization/)\n",
    "- Shubhang Agrawal on [image segmentation using K-means clustering](https://medium.com/swlh/image-segmentation-using-k-means-clustering-46a60488ae71). (The tutorial has a few flaws, please excuse). \n",
    "\n",
    "\n",
    "<!-- \n",
    "Resources:\n",
    "# Nice way of depicting the bars\n",
    "https://pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/\n",
    "# OpenCV\n",
    "https://docs.opencv.org/3.4/d1/d5c/tutorial_py_kmeans_opencv.html\n",
    "# Machine Learning Mastery\n",
    "https://machinelearningmastery.com/k-means-clustering-in-opencv-and-application-for-color-quantization/\n",
    "# Watershed\n",
    "https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "# Segmentation with Skimage \n",
    "https://github.com/ipython-books/cookbook-2nd-code/blob/master/chapter11_image/03_segmentation.ipynb\n",
    "# Combination between thresholding and color clustering\n",
    "https://towardsdatascience.com/image-color-segmentation-by-k-means-clustering-algorithm-5792e563f26e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Choose here the image to work with\n",
    "img = img1\n",
    "\n",
    "# 1) Reshape the color pixels into a Mx3 matrix (M: number of pixels)\n",
    "#    and convert the data type to float32.\n",
    "data = img.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "# 2) Apply the K-means algorithm to the data. Use the cv.kmeans function.\n",
    "#    Choose the number of clusters K=3.\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "K = 3\n",
    "ret, label, centers = cv.kmeans(data, K, None, criteria, 10, cv.KMEANS_RANDOM_CENTERS)\n",
    "# label contains the cluster index for each pixel\n",
    "# centers contains the cluster centers (colors!)\n",
    "\n",
    "# 3) Reshape and convert the data back to uint8\n",
    "img_seg = ...\n",
    "\n",
    "# 4) Visualize the segmented image\n",
    "tools.show_image_pair(img, img_seg, title1=\"Original\", title2=\"Segmented\");\n",
    "\n",
    "# 5) Repeat the process for a different color space (e.g. HSV)\n",
    "#    Is the clustering more robust? Why? When does this approach fail?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "def segment_blood_cells_kmeans(img, K=3, use_lab=False):\n",
    "    # Blur the image to reduce noise (step is required here \n",
    "    # to yield feasible results)\n",
    "    img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    if use_lab:\n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2LAB)\n",
    "\n",
    "    # 1) Reshape the color pixels into a Mx3 matrix (M: number of pixels)\n",
    "    #    and convert the data type to float32.\n",
    "    data = img.reshape(-1, 3).astype(np.float32)\n",
    "    \n",
    "    # 2) Apply the K-means algorithm to the data. Use the cv.kmeans function.\n",
    "    # Some parameters for the kmeans algorithm (termination criteria):\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
    "    ret, label, centers = cv.kmeans(data, \n",
    "                                    K=K, \n",
    "                                    bestLabels=None, \n",
    "                                    criteria=criteria, \n",
    "                                    attempts=10, \n",
    "                                    flags=cv.KMEANS_PP_CENTERS)\n",
    "    # label contains the cluster index for each pixel\n",
    "    # centers contains the cluster centers (colors!)\n",
    "\n",
    "    # 3) Reshape and convert the data back to uint8\n",
    "    img_seg = centers[label.flatten()].reshape(img.shape).astype(np.uint8)\n",
    "\n",
    "    if use_lab:\n",
    "        img_seg = cv.cvtColor(img_seg, cv.COLOR_LAB2RGB)\n",
    "\n",
    "    # 4) Return the segmented image\n",
    "    return img_seg\n",
    "\n",
    "\n",
    "img = img1\n",
    "img_seg = segment_blood_cells_kmeans(img, K=3, use_lab=False)\n",
    "tools.show_image_chain([img, img_seg], titles=[\"Original\", \"Segmented\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here looks somewhat better than in the previous example; however, the segmentation is still not perfect:\n",
    "\n",
    "- The effectiveness of clustering depends on the prevalence of colors in the image. If the background contains many subtle color variations, K-means may allocate more clusters to the background rather than focusing on the blood cells.\n",
    "- The K-means algorithm is sensitive to the initialization of cluster centers. Poor initialization can lead to convergence to a local minimum, resulting in suboptimal clustering.\n",
    "- The method still struggles with touching or overlapping cells. While the cells may be assigned the correct color, they are not individually separated.\n",
    "\n",
    "We may improve the results by switching to a different color space. For instance, the LAB color space is more robust to illumination changes and better reflects human color perception. Another strategy is to allow the algorithm to identify more clusters, and then apply post-processing to merge similar clusters. For example, if several cluster centers have a dominant blue component, they could be combined into a single class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Method 3: Watershed algorithm for segmentation**\n",
    "\n",
    "The watershed algorithm is a powerful tool for image segmentation, particularly useful for separating complex or overlapping structures. It is based on the concept of watershed lines, which define boundaries between different regions in an image. The algorithm works by conceptually \"flooding\" the image from predefined seed points. As water spreads from each seed region, it continues flowing until it meets water from a neighboring region.  \n",
    "The boundaries where the regions meet are defined as the watershed lines, effectively separating the regions. Watershed segmentation can be applied based on pixel intensity or color and is especially effective for images with intricate shapes and touching objects.\n",
    "\n",
    "Our dataset has structural similarities to the image used in this [OpenCV watershed tutorial](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html). We will now follow a simplified version of the tutorial to segment our images using the watershed method. The tutorial uses the following strategy:\n",
    "\n",
    "### **Overview / Steps**\n",
    "\n",
    "1. Convert the image to a binary mask using thresholding.\n",
    "2. Apply morphological operations to reduce noise and help separate objects. These operations are also used to identify regions that likely represent the background.\n",
    "3. Identify seed points for the watershed algorithm:\n",
    "   - Apply the distance transform to the binary mask. This computes, for each pixel, the distance to the nearest background pixel (value 0).\n",
    "   - Threshold the distance-transformed image to isolate blobs near the centers of objects of interest.\n",
    "   - Use the connected components algorithm to label and enumerate the seed regions, creating a labeled mask.\n",
    "   - Mark the background seed region (identified in step 2) with label `0`.\n",
    "4. Apply the watershed algorithm to segment the regions.\n",
    "5. Visualize the resulting segmentation.\n",
    "\n",
    "\n",
    "\n",
    "### **Note: Morphological operations**\n",
    "\n",
    "Morphological operations are a set of operations used to analyze and manipulate the shape of objects in an image. Although they are defined for various image types, they are most commonly applied to binary images. These operations involve a structuring element (kernel) that probes the image and modifies pixel values based on the interaction between the kernel and the image. The most common operations include dilation (expands shapes), erosion (shrinks shapes), opening (erosion followed by dilation), and closing (dilation followed by erosion). Morphological operations are useful for removing noise, separating objects, and connecting disjoint regions in an image. There is a separate notebook on morphological operations.\n",
    "\n",
    "**Further reading**:  \n",
    "- OpenCV documentation: [Link](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html)  \n",
    "- Beautiful illustration of morphological operations: [Link](https://penny-xu.github.io/blog/mathematical-morphology)  \n",
    "- Wikipedia article on mathematical morphology: [Link](https://en.wikipedia.org/wiki/Mathematical_morphology)  \n",
    "- Blog post on morphological operations: [Link](https://towardsdatascience.com/7bcf1ed11756)\n",
    "\n",
    "### **Note: Distance transform**\n",
    "The distance transform is useful for various image processing tasks. It computes the distance from each pixel to the nearest boundary (i.e., the closest background pixel) in a binary image. Distance transforms are used for operations like skeletonization, shape analysis, and segmentation. The algorithm propagates distance values from boundary pixels inward, typically using a metric such as the Euclidean distance. It is computationally efficient and widely available in image processing libraries.\n",
    "\n",
    "**Further reading**:  \n",
    "- Application of distance transform with watershed: [Link](https://docs.opencv.org/3.4/d2/dbd/tutorial_distance_transform.html)\n",
    "\n",
    "### **Note: Connected components**\n",
    "\n",
    "Connected components are regions in a binary image where pixels are connected based on predefined neighborhood rules (e.g., 4-connectivity or 8-connectivity). This technique is used to identify individual objects in a segmentation mask. The algorithm labels each connected region with a unique integer value, allowing for further analysis such as counting or measuring properties of each component.\n",
    "\n",
    "**Further reading**:  \n",
    "- Wikipedia article on connected component labeling: [Link](https://en.wikipedia.org/wiki/Connected-component_labeling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "img = img1\n",
    "\n",
    "# Implement the approach lined out above. You can copy paste the code \n",
    "# from the above link and plug our image into it. Try to understand the\n",
    "# code and the different steps. You may have to adjust the parameters\n",
    "# to get a good segmentation result.\n",
    "\n",
    "# https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "def segment_red_blood_cells_watershed(img):\n",
    "    \n",
    "    img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "    img_blur = cv.medianBlur(img, 5)\n",
    "    \n",
    "    #tools.show_image(img_blur)\n",
    "    \n",
    "    gray = cv.cvtColor(img_blur, cv.COLOR_RGB2GRAY)\n",
    "    gray = img_blur[:,:,0]\n",
    "    ret, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "\n",
    "    # Noise removal\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations=9)\n",
    "\n",
    "    # Sure background area\n",
    "    sure_bg = cv.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "    thr = 0.1 * dist_transform.max()\n",
    "    thr = 18\n",
    "    ret, sure_fg = cv.threshold(dist_transform, thr, 255, 0)\n",
    "    \n",
    "    tools.show_image_chain([sure_fg, sure_bg], titles=[\"Sure FG\", \"Sure BG\"])\n",
    "    tools.show_image_chain([opening, dist_transform], titles=[\"Opening\", \"Distance transform\"])\n",
    "\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    # Marker labelling\n",
    "    ret, markers = cv.connectedComponents(sure_fg)\n",
    "    \n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers+1\n",
    "    \n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown==255] = 0\n",
    "    \n",
    "    markers = cv.watershed(img,markers)\n",
    "    img[markers == -1] = [255,0,0]\n",
    "    \n",
    "    return markers, img\n",
    "    \n",
    "    \n",
    "img = img1.copy()\n",
    "markers, result = segment_red_blood_cells_watershed(img=img)\n",
    "tools.show_image_chain([markers, result], \n",
    "                       titles=[\"Markers\", \"Segmented\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **AI driven segmentation**\n",
    "Deep learning is increasingly used for image segmentation tasks, with the U-Net architecture still being one of the most popular choices. U-Net is a convolutional neural network specifically designed for biomedical image segmentation. \n",
    "\n",
    "A specialized U-Net-based tool for medical imaging is [nnU-Net](https://github.com/MIC-DKFZ/nnUNet). It features self-configuring preprocessing and postprocessing, allowing the network to automatically adapt to the characteristics of the input data. nnU-Net is available as a Python package and can be installed via pip.\n",
    "\n",
    "Although machine learning and AI are not the core focus of this course, pre-trained models can still be applied to perform segmentation effectively. Unlike classical methods, deep learning models can learn features directly from the data and often generalize better to unseen examples. However, they require large labeled datasets for training, are more computationally demanding, and are often seen as \"black boxes\" â€“Â making it difficult to interpret their decisions.\n",
    "\n",
    "\n",
    "```python\n",
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "```\n",
    "\n",
    "Visit the following resources and explore whether they could be useful for your own segmentation project:\n",
    "\n",
    "- **Segment Anything** by Meta AI [Demo](https://segment-anything.com/demo), [Paper](https://arxiv.org/abs/2304.02643), [Code](https://github.com/facebookresearch/segment-anything) \n",
    "- **Huggingface**: Collection of public pre-trained models. [Link](https://huggingface.co/models).\n",
    "  - Many models include a demo interface\n",
    "  - Background removal with [RemBG](https://huggingface.co/spaces/KenjieDec/RemBG)\n",
    "  - Another popular segmentation tool is [YOLO](https://huggingface.co/spaces/fcakyon/yolov8-segmentation) ([Code](https://huggingface.co/spaces/fcakyon/yolov8-segmentation))\n",
    "  - To search the entire Huggingface database for models: [Link](https://huggingface.co/models)\n",
    "- **TotalSegmentator** for anatomical CT (and MR) segmentation. [Demo](https://totalsegmentator.com/), [Paper](https://arxiv.org/abs/2208.05868), [Code](https://github.com/wasserth/TotalSegmentator)\n",
    "\n",
    "\n",
    "We have now explored several approaches to image segmentation.  How well you can apply them will depend on your specific problem â€“ and a bit of engineering skill. ðŸ˜Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Try using one of the models listed above to segment the cells in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# Let's use the Segment Anything model from Meta.\n",
    "# The following lines may take a while to execute.\n",
    "try:\n",
    "    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "except ImportError:\n",
    "    print(\"Installing the model...\")\n",
    "    !pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
    "    !pip install -q opencv-python pycocotools matplotlib onnxruntime onnx\n",
    "    !pip install -q torch torchvision\n",
    "\n",
    "# Download the model (if not available yet)\n",
    "path_to_checkpoint=\"./sam_vit_h_4b8939.pth\"\n",
    "path_to_checkpoint=\"/Users/juch/workspace/education/phd/data/models/sam_vit_h_4b8939.pth\"\n",
    "url_checkpoint=\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "if not Path(path_to_checkpoint).exists():\n",
    "    print(\"Downloading model... This may take a while!\")\n",
    "    !wget -O {path_to_checkpoint} {url_checkpoint}\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=path_to_checkpoint)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(img, overlay):\n",
    "    \"\"\"Blend an image with an overlay.\"\"\"\n",
    "    alpha = overlay[:,:,3]\n",
    "    img = img.astype(np.float32)\n",
    "    result = ((1 - alpha[:, :, None]) * img + \n",
    "              alpha[:, :, None] * overlay[:, :, :3] * 255)\n",
    "    result = result / result.max()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_blood_cells_sam(img, masks):\n",
    "    result = img.copy()\n",
    "    overlay_color = [255, 255, 0]\n",
    "    alpha = 0.2\n",
    "    clean_masks = True\n",
    "\n",
    "    # Sort masks by area\n",
    "    masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "\n",
    "    if clean_masks:\n",
    "        # Filter masks that are fully mask\n",
    "        to_remove = []\n",
    "        for i, m1 in enumerate(masks):\n",
    "            m1 = m1[\"segmentation\"]\n",
    "            for j in range(i+1, len(masks)):\n",
    "                m2 = masks[j][\"segmentation\"]\n",
    "                if i in to_remove or j in to_remove:\n",
    "                    continue\n",
    "                if (m1.sum()) ==( (m1 | m2).sum()):\n",
    "                    to_remove.append(j)\n",
    "        masks = [m for i, m in enumerate(masks) if i not in to_remove]\n",
    "        \n",
    "    # Check the type of cell, using the following heuristic:\n",
    "    # If the mask is mostly red, it is a red blood cell, if\n",
    "    # it is mostly blue, it is a white blood cell\n",
    "    for m in masks:\n",
    "        mask = m[\"segmentation\"]\n",
    "        r = img[:,:,0][mask].mean()\n",
    "        b = img[:,:,2][mask].mean()\n",
    "        m[\"type\"] = \"rbc\" if r > b else \"wbc\"\n",
    "        \n",
    "    # Visualize the masks\n",
    "    result = np.ones((img.shape[0], img.shape[1], 4))\n",
    "    result[:,:,3] = 0\n",
    "    for m in masks:\n",
    "        cell = m[\"type\"]\n",
    "        m = m[\"segmentation\"]\n",
    "        contours, hierarchy = cv.findContours(m.astype(np.uint8)*255, \n",
    "                                                cv.RETR_TREE, \n",
    "                                                cv.CHAIN_APPROX_SIMPLE)\n",
    "        # Random numbers for the color\n",
    "        rr, rb, rg = np.random.random(3)*0.5\n",
    "        overlay_color = ([255/255, rb, rg, alpha] if (cell == \"rbc\") \n",
    "                         else [rr, rg, 255/255, alpha])\n",
    "        result[m] = overlay_color\n",
    "        overlay_color[3] = 1\n",
    "        cv.drawContours(result, contours, -1, overlay_color, 2)\n",
    "        \n",
    "    result = blend(img, result)\n",
    "    tools.show_image_chain([img, result], titles=[\"Input\", \"Output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image here:\n",
    "img = img1\n",
    "\n",
    "#Â Generate the masks (this may take a while, about 30s)\n",
    "# Keeping this call outside the segment* function because \n",
    "# this step takes a while.\n",
    "print(\"Generating mask... This may take a while!\")\n",
    "masks = mask_generator.generate(img)\n",
    "\n",
    "# Visualize the masks\n",
    "segment_blood_cells_sam(img, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mask for the red blood cells and save it to a file\n",
    "mask = sum([m[\"segmentation\"] for m in masks if m.get(\"type\") == \"rbc\"])\n",
    "# Apply some morphological opening to clean the mask\n",
    "mask = cv.morphologyEx(mask.astype(np.uint8), cv.MORPH_OPEN, \n",
    "                       np.ones((3, 3), np.uint8), iterations=1)\n",
    "tools.show_image(mask.astype(np.uint8)*255, title=None, suppress_info=True)\n",
    "cv.imwrite(\"mask_rbc.png\", mask.astype(np.uint8)*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks good â€“ the model segments the different cells with high accuracy. This is impressive given that the Segment Anything model was trained on a general-purpose dataset rather than on hematologic images. This suggests that Anything model generalizes well to new domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for img2\n",
    "img = img2\n",
    "masks = mask_generator.generate(img)\n",
    "segment_blood_cells_sam(img, masks)\n",
    "\n",
    "# Repeat for img3\n",
    "img = img3\n",
    "masks = mask_generator.generate(img)\n",
    "segment_blood_cells_sam(img, masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
